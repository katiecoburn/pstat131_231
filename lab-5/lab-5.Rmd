---
title: "Lab 5: Regularized Regression"
author: "PSTAT 131/231"
output:
    html_document:
      toc: true
      toc_float: true
      code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

## Introduction

This lab will take a look at (1) regularized regression models and (2) their corresponding hyperparameter tuning for both **linear** and **logistic** regression, along with (3) methods for handling imbalanced categorical outcomes like **upsampling** and **downsampling**.

This chapter will use ['parsnip'](https://www.tidymodels.org/start/models/) for model fitting and ['recipes and workflows'](https://www.tidymodels.org/start/recipes/) to perform the transformations, and ['tune and dials'](https://www.tidymodels.org/start/tuning/) to tune the hyperparameters of the model. All these packages are automatically installed and loaded along with the `tidymodels` suite of packages.

Note that regularization -- ridge and lasso, etc. -- can be used for **both linear and logistic regression**.

### Loading Packages

We load `tidymodels` for modeling functions, `ISLR` and `ISLR2` for data sets, and the `tidyverse`. We also need to load the `glmnet` package, since we'll use it as the engine for the regularized regressions.

```{r}
library(tidymodels)
library(ISLR)
library(ISLR2)
library(tidyverse)
library(glmnet)
library(modeldata)
library(ggthemes)
library(janitor) # for naming conventions
library(naniar) # to assess missing data patterns
library(corrplot) # for a correlation plot
library(themis) # for upsampling
tidymodels_prefer()
```

Notice that we usually include `tidymodels_prefer()` after loading all these libraries. That isn't required, but it can be useful because R gives packages priority according to the order in which they were loaded, meaning that the package you've loaded most recently can "overwrite" a function from a previous package with the same name. This phenomenon of multiple functions with the same name(s) from different packages is called a "conflict." `tidymodels_prefer()` is a one-line way of handling any `tidyverse` or `tidymodels`-related conflicts by telling R to use the functions from those groups of packages if a conflict should arise.

### Data

We'll be working with two data sets for this lab, one to illustrate a regression problem and one to illustrate a classification problem. Let's take a look at each one.

#### Regression: Baseball salaries

For regression, we'll use the `Hitters` data set; `Hitters` is included in the `ISLR` package, so we can use it once we've loaded that package. Our goal to predict baseball players' `Salary` based on several different characteristics which are included in the data set, like their number of times at bat, number of hits, division, etc.

We'll turn the data set into a tibble (the `tidyverse` version of a data frame) and use `head()` to view the first few rows:

```{r}
hitters <- as_tibble(Hitters)
head(hitters)
```

This is as good a time as any to introduce the `janitor` package. The `janitor` package contains a number of helper functions that can expedite parts of data cleaning; however, personally, I find myself most often using its `clean_names()` function. Try running `?clean_names` to see if you can determine what the function does before reading on.

When we download or otherwise obtain data that is "real" (as in not simulated), the column names, or variable names, may not all be in the same [case](https://www.freecodecamp.org/news/programming-naming-conventions-explained/), or they may be in a case that isn't easy to work with. It can be helpful to change all column names so that they follow one universal [naming convention](https://www.freecodecamp.org/news/programming-naming-conventions-explained/). We can do that with `clean_names()`, which by default will make all columns snake case. (Snake case means all words start with lowercase letters and are separated by underscores, which makes them look_like_little_snakes.)

```{r}
hitters <- hitters %>% 
  clean_names()
head(hitters)
```

We start, as normal, by splitting the data into training and testing sets using stratified sampling; we'll also use *k*-fold cross-validation with $k = 10$ to fold the training set.

```{r}
set.seed(3435)
hitters_split <- initial_split(hitters, strata = "salary", prop = 0.75)

hitters_train <- training(hitters_split)
hitters_test <- testing(hitters_split)

hitters_fold <- vfold_cv(hitters_train, v = 10)
```

We can visualize missingness within the data -- let's look at the whole data set prior to splitting to see the complete amount:

```{r}
vis_miss(hitters)
```

And let's visualize a correlation matrix of the numeric variables:

```{r}
hitters_train %>% 
  select(is.numeric) %>% # selecting numeric columns
  cor(use = "pairwise.complete.obs") %>% # handling missing data in Salary
  corrplot(type = "lower", diag = FALSE) # printing lower half of matrix
  # and not including the correlations of 1 on the diagonal
```

There seem to be strong positive linear correlations between `years`, `c_hits`, `c_runs`, `crbi`, and `c_walks`. There are also strong positive correlations between `at_bat`, `hits`, `runs`, `rbi`, and `walks`. We'll try including `step_pca()` to combat this collinearity.

#### Activities

-   Access the help page for `Hitters`. Familiarize yourself with the predictor variables and any other useful information about the data.
-   What variables are correlated with `Salary`?
-   How do you think you might choose to handle the missing data in this data set, and why might you choose to handle it that way?
-   Are there any correlations in the data that surprised you? Why or why not?

We'll set up a recipe for the regression problem first. Note that we `prep()` and `bake()` the recipe after setting it up. This is not necessary, but is almost always recommended; without doing so, we wouldn't notice any errors in the recipe until the models were fit, and at that point it could be hard to trace the errors back.

The majority of the time, errors that we encounter in code during this course are related, in some way, to an error or misspecification in the recipe.

```{r}
hitters_recipe <- recipe(salary ~ . , data = hitters_train) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_predictors()) %>% 
  step_impute_linear(salary, 
                     impute_with = imp_vars(c_hits, c_runs, crbi, c_walks, c_at_bat),
                     skip = TRUE) %>% 
  step_pca(hits, hm_run, runs, rbi, walks, 
           num_comp = 1, prefix = "first_pc") %>% 
  step_pca(years, c_hits, c_runs, crbi, c_walks, c_at_bat,
           num_comp = 1, prefix = "second_pc")

prep(hitters_recipe) %>% bake(new_data = hitters_train)
```

We normalize all the predictors first, as is generally recommended when conducting PCA, so that they are all on the same scale. (PCA will be discussed in more depth later in the course.) We then extract two principal components, one representing all the common information in the first group of five correlated predictors and the second all the information in the second group.

#### Classification: Customer churn

For classification, we'll use the `mlc_churn` data set, which is part of the `modeldata` package. It contains a simulated data set that is provided along with certain machine learning software for practice predicting customer churn. Customer churn is a specific name for customer attrition, or whether or not customers stop doing business with an entity or organization. It's generally in a business' best interest to retain as many customers as possible and minimize churn, so a model that can accurately predict when customers will stop doing business somewhere is very useful.

Again, note that this is simulated data; it's somewhat difficult to find real data on churn rates that is freely available, likely because businesses may be reluctant to make that data public. This data represents churn at a **phone company**, as indicated by the predictors related to things like daytime minutes and calls, nighttime minutes and calls, and international minutes and calls, etc. The outcome variable is `churn`.

We'll read the data in and use `head()` to view the first few rows. We don't need to use `clean_names()` here; the columns are already named consistently, using snake case.

```{r}
mlc_churn <- modeldata::mlc_churn %>% 
  as_tibble()

mlc_churn %>% 
  head()
```

Normally it would be useful to check for missing data, but the data are simulated and we actually already know none of it is missing. However, you can try running `vis_miss()` anyway to verify.

Let's look at a correlation matrix:

```{r}
mlc_churn %>% 
  select(is.numeric) %>% # selecting numeric columns
  cor() %>%
  corrplot(type = "lower", diag = FALSE)
```

We can see something interesting here -- the only strong linear correlations are between each of the `_charge` variables and their corresponding `_minutes` variables, and these correlations are very strong. In fact, if you look at `cor()` directly or add `method = "number"` to `corrplot()`, you can see that these correlations are exactly $1.00$.

We'll handle this collinearity by simply excluding one of each of these variable pairs from our recipe. This is also a good opportunity to drop `state` and `area_code`, since there are a lot of levels of each, making it a little tricky to format them for inclusion in the models. The next code chunk removes these variables from the data entirely and does the splitting and folding:

```{r}
set.seed(3435)
mlc_churn <- mlc_churn %>% 
  select(-c(total_day_charge, total_eve_charge, total_night_charge,
            total_intl_charge))
mlc_split <- initial_split(mlc_churn, strata = "churn", prop = 0.75)

mlc_train <- training(mlc_split)
mlc_test <- testing(mlc_split)

mlc_fold <- vfold_cv(mlc_train, v = 10)
```

It's also useful to look at the distribution of the outcome for our classification problem in particular:

```{r}
mlc_train %>% 
  ggplot(aes(x = churn)) + geom_bar() +
  theme_bw()
```

There is definitely a class imbalance here. We can get the exact proportions:

```{r}
mlc_train %>% 
  group_by(churn) %>% 
  summarise(prop = n()/(dim(mlc_train)[1]))
```

Approximately $86\%$ of customers in the training data set did not churn, while only $14\%$ of them did. This imbalance will likely make it difficult for the model to learn to predict `yes` accurately, so we'll adjust for it later.

Next, let's set up the recipe for this data!

We would start out typically by dummy-coding the nominal predictors, but notice that, if we do so and prep and bake, we've increased the number of columns from 16 to 66. This is because there are 50 states in the USA and there is a categorical variable representing state.

```{r}
churn_recipe <- recipe(churn ~ ., data = mlc_train) %>% 
    step_dummy(all_nominal_predictors())

prep(churn_recipe) %>% bake(new_data = mlc_train)
```

One way we could choose to handle this is by not including `state` as a predictor at all. However, it might seem plausible that customer churn differs by state. Let's take a look:

```{r}
mlc_train %>% 
  ggplot(aes(y = forcats::fct_infreq(state))) +
  geom_bar() +
  theme_base() +
  ylab("State")
```

There are at least 30 customers in each state; one solution without including 49 dummy variables is to collapse the number of levels, essentially grouping some of the states together. To read more about ways of handling categorical variables, I strongly recommend investigating the documentation of the `tidyverse` package `forcats` [here](https://forcats.tidyverse.org/).

We could use `fct_lump()` to combine the less common levels, but here most levels of the variable actually have a fair number of observations, so that probably doesn't make as much sense. A more effective alternative might be to recode this variable to represent **regions** of the US rather than individual states -- meaning:

![Figure 1: US state census regions.](https://cdn.mappr.co/wp-content/uploads/2021/12/us-regions-map-census.jpg)

`fct_collapse()` is a good function for this; see its [documentation and example code](https://forcats.tidyverse.org/reference/fct_collapse.html). It would likely be easier to do this to the entire data set before splitting and folding it, so (unfortunately) we'll repeat the splitting part again below. We don't **have** to do it in this order, but if we didn't we'd have to manually recode the testing set in order to use our winning model on it.

```{r}
set.seed(3435)
mlc_churn <- mlc_churn %>%
  mutate(region = forcats::fct_collapse(state,
                                        west = c("CA", "OR", "WA", "ID",
                                                 "MT", "NV", "WY", "UT",
                                                 "CO", "AZ", "NM", "AK",
                                                 "HI"),
                                        midwest = c("ND", "SD", "NE", "KS",
                                                    "MN", "IA", "MO", "WI",
                                                    "IL", "MI", "IN", "OH"),
                                        northeast = c("ME", "VT", "NH", "MA",
                                                      "CT", "RI", "NY", "PA",
                                                      "NJ"),
                                        south = c("TX", "OK", "AR", "LA",
                                                  "KY", "TN", "MS", "AL",
                                                  "WV", "DE", "MD", "VA",
                                                  "NC", "SC", "GA", "FL",
                                                  "DC"))) %>% 
  select(-state)
mlc_split <- initial_split(mlc_churn, strata = "churn", prop = 0.75)

mlc_train <- training(mlc_split)
mlc_test <- testing(mlc_split)

mlc_fold <- vfold_cv(mlc_train, v = 10)
```

Let's confirm that our factor recoding worked:

```{r}
mlc_train %>% 
  ggplot(aes(y = forcats::fct_infreq(region))) +
  geom_bar() +
  theme_base() +
  ylab("Region")
```

Excellent -- we now have a variable called `region` that represents the general census region for each value, meaning we can dummy code this variable with only 3 dummy variables, not 49. Back to the recipe.

We do want to handle the imbalance in the levels of the outcome; as we saw earlier, many more customers do not churn than do, and if we train our models on a severely imbalanced data set, they can accidentally become much better at identifying one level versus the other. Companies probably care much more about correctly identifying who **will** take their business elsewhere versus who **won't**.

We'll use the `step_upsample()` function from [the `themis` package](https://github.com/tidymodels/themis) for this. Downsampling is also a reasonable option, and actually would probably be fine here since there are a relatively large number of observations, but for most students working on their course projects, losing observations will be somewhat costly and upsampling might be a better option.

Note that the `step_upsample()` and `step_downsample()` functions have an argument of `skip = TRUE` set by default, meaning that their results won't be applied to `bake()` unless you manually set `skip = FALSE`. Here we'll do that first just to demonstrate that it's brought the counts to be equal, and then rewrite the recipe without.

We'll use `over_ratio = 0.5` here so that there will be approximately half as many `yes`s as there are `no`s. The default is `over_ratio = 1`, but that can also result in overfitting problems, so there is a tradeoff involved.

```{r}
churn_recipe_demo <- recipe(churn ~ ., data = mlc_train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_predictors()) %>% 
  step_upsample(churn, over_ratio = 0.5, skip = FALSE)

prep(churn_recipe_demo) %>% bake(new_data = mlc_train) %>% 
  group_by(churn) %>% 
  summarise(count = n())

churn_recipe <- recipe(churn ~ ., data = mlc_train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_predictors()) %>% 
  step_upsample(churn, over_ratio = 1)
```

## Elastic Net Regression

`parsnip` does not have a dedicated function to create a ridge regression model specification; we need to use **either** `linear_reg()` or `logistic_reg()` and set `mixture = 0` to specify a ridge model. The `mixture` argument specifies the amount of different types of regularization; `mixture = 0` specifies only ridge regularization and `mixture = 1` specifies only lasso regularization. Setting `mixture` to a value between 0 and 1 lets us use both, which results in a mixture of L1 and L2 regularization, or what is often called an "elastic net."

The nice thing is that by specifying a range of values for `mixture` between 0 and 1, **inclusive**, we can essentially fit both ridge, lasso, **and** a variety of elastic net models at the same time, considering all our values of `penalty` at the same time! That's what we'll do here.

When using the `glmnet` engine, we also need to set a penalty to be able to fit the model. This `penalty` argument corresponds to $\lambda$ in the slides and textbook. Generally, we'll select the optimal value of $\lambda$ by tuning; its value can essentially range between \$0\$, which is equivalent to traditional linear or logistic regression, and an upper limit of positive infinity.

```{r}
en_spec_hitters <- linear_reg(mixture = tune(), 
                              penalty = tune()) %>%
  set_mode("regression") %>%
  set_engine("glmnet")

en_spec_churn <- logistic_reg(mixture = tune(), 
                              penalty = tune()) %>%
  set_mode("classification") %>%
  set_engine("glmnet")
```

The `tune()` piece of code here flags those values for `tidymodels` as something that we'll be trying out different values for. If we try looking at a model specification, like below:

```{r}
en_spec_hitters
```

we see that penalty and mixture have each been set equal to `tune()`. If we then did not ultimately use `tune_grid()`, R would complain and tell us we needed to either set these arguments to one specific value each or specify ranges of possible values to try.

### Hyperparameter Tuning

Hyperparameter tuning is, in its simplest form, a way of fitting many models with different sets of hyperparameters trying to find one that performs "best." The complexity in hyperparameter tuning often comes from the number of different models tried. We will keep it simple for this lab and use an evenly-spaced grid method of searching a range of parameter values, only looking at evenly spaced parameter values.

This is a fine approach if you have one or two hyperparameters, as we do here, and/or with relatively simple models (like regularized regression), but with a larger number of hyperparameters and a large number of levels, it can become computationally infeasible. One solution is to reduce the number of levels or tune fewer hyperparameters (either of these are what we'll usually do), but if you're interested in other options, see the chapter on ['iterative search'](https://www.tmwr.org/iterative-search.html) from ['Tidy Modeling with R'](https://www.tmwr.org/) for more information.

We can use the `tune_grid()` function to perform hyperparameter tuning using a grid search. `tune_grid()` needs 3 different things:

1.  a `workflow` object containing the model and preprocessor
2.  an `rset` object containing the resamples the workflow should be fitted within;
3.  and a tibble containing the parameter values to be evaluated.

Optionally, a metric set of performance metrics can be supplied for evaluation. If you don't set one, then a default set of performance metrics is used; for regression problems this is typically a set of `rmse` and `rsq`, for classification it is typically a set of `accuracy` and `roc_auc`.

We already have `resample` objects created (we made these by folding the data sets) and workflow specifications; now we'll combine them for each data set to create `workflow` objects.

```{r}
en_workflow_churn <- workflow() %>% 
  add_recipe(churn_recipe) %>% 
  add_model(en_spec_churn)

en_workflow_hitters <- workflow() %>% 
  add_recipe(hitters_recipe) %>% 
  add_model(en_spec_hitters)
```

The last thing we need is the values of `penalty` and `mixture` we are trying. These can be created using `grid_regular()`, which creates a grid of evenly spaced parameter values. We use the `penalty()` function, which comes from the `dials` package within `tidymodels`, to denote the parameter and set the range of the grid we are searching for. Note that this range is log-scaled by default; we use `trans = identity_trans()` to tell R that we want to use the exact values that we specified, from 0 to 1. We'll ask for 10 levels.

For `mixture`, we'll use the `mixture()` function and set it up in virtually the same way, although the range for this function isn't automatically log-scaled, so we don't need the `trans` argument.

```{r}
en_grid <- grid_regular(penalty(range = c(0, 1),
                                     trans = identity_trans()),
                        mixture(range = c(0, 1)),
                             levels = 10)
```

Now we have everything we need and we can fit all the models. The following code chunk is the one that will take the **longest** to run in this entire document, because it is the one that involves fitting the most models, first to the MLB data and then to the data on customer churn.

```{r}
tune_res_hitters <- tune_grid(
  en_workflow_hitters,
  resamples = hitters_fold, 
  grid = en_grid
)

tune_res_churn <- tune_grid(
  en_workflow_churn,
  resamples = mlc_fold, 
  grid = en_grid
)

tune_res_hitters
```

The output of `tune_grid()` can be hard to read by itself, unprocessed (although it's shown above for the MLB data). It is what's known as a "nested tibble" and contains four columns, each with 10 rows.

The first two columns are easier to understand -- `splits`, which contains the information necessary to recreate the assessment and analysis subsets for each resample object, and `id`, which identifies what fold each row corresponds to.

The next column, `.metrics`, is essentially a list of 10 tibbles, where each tibble contains 200 rows, equal to the number of models being fit per fold (100, the number of hyperparameter combinations) times the number of metrics being calculated (2 for each data set, either `rmse` and `rsq` or `accuracy` and `roc_auc`). You can run the code `tune_res_hitters$.metrics` to view it directly.

Finally, `.notes` is just a list of 10 tibbles that contains any warnings or error messages generated while fitting all the models to all the folds. If nothing went wrong, it's usually a list of empty 0 by 3 tibbles. If something did go wrong, you can use the `.notes` column to extract any specific error message. You can run the code `tune_res_hitters$.notes` to access **it** directly.

`autoplot()` creates a great visualization of the information about each metric across each fold for us, first for MLB then for churn:

```{r}
autoplot(tune_res_hitters)
```

```{r}
autoplot(tune_res_churn)
```

We'll demonstrate how to look at/interpret these results for the MLB data, and then suggest that you practice interpreting the customer churn results.

For the MLB data, the scale of the y-axis for both metrics is relatively small, first of all. This indicates that the resulting performance doesn't really vary drastically across any of the models we've fit. The amount of regularization, on the x-axis, is the penalty hyperparameter, which covers the range of values we specified (zero to one), and the values of mixture are represented by the different-colored lines.

Overall, the models with zero percentage of lasso penalty, or the ridge regression models, do better, as indicated by the red line being consistently higher (or lower) than the others. This implies that it yields better performance to avoid reducing predictors all the way down to zero, as can happen in the case of lasso regression. Models with some non-zero proportion of lasso start to do slightly better as the penalty value increases, although still their performance is not near the level of ridge regression.

We can also see the raw metrics that created each of these charts, averaged across all 10 folds, by calling `collect_metrics()` -- first for MLB then for churn:

```{r}
collect_metrics(tune_res_hitters)
```

```{r}
collect_metrics(tune_res_churn)
```

#### Activities

-   How does `en_grid` change if you set `levels = 100`? `levels = 50`?
-   Investigate the other `grid` functions in `tidymodels`. Try one of them out here. What do you notice?
-   How many models are we fitting for each individual data set here?
-   Interpret the tuning results for the customer churn data set. What general patterns do you see?

### Model Selection

The "best" values of these can be selected using `select_best()`, or using any variation on that function like `select_by_one_std_error()`, etc. These functions require you to specify a metric that it should use to select, so we'll use `rmse` for the MLB data and `roc_auc` for the churn data.

```{r}
best_en_hitters <- select_by_one_std_err(tune_res_hitters,
                          metric = "rmse",
                          penalty,
                          mixture
                          )
best_en_hitters
```

The best hyperparameter combination for the MLB data uses a penalty value of 0, meaning that it's basically traditional linear regression, and a mixture value of 0, meaning that no lasso penalty is present.

```{r}
best_en_churn <- select_by_one_std_err(tune_res_churn,
                          metric = "roc_auc",
                          penalty,
                          mixture
                          )
best_en_churn
```

For the customer churn data, the best hyperparameter combination actually also uses penalty and mixture values of 0, which is essentially a logistic regression.

#### Activities

-   Explore what happens when you use `select_best()` instead of `select_by_one_std_error()`.

-   Do you think ascending or descending values of `penalty` and `mixture` are more parsimonious? Explain why.

-   Assuming we go on to fit these models as described, explain in your own words what the functional difference(s) are between them and typical linear or logistic regression.

### Model Generalizability

These values of penalty and mixture can then be used with `finalize_workflow()` to update/finalize the recipes by replacing `tune()` with the value of `best_en_hitters` and `best_en_churn`. Now, these **best** models should be fit again, this time using the **whole training data set**.

```{r}
en_final_hitters <- finalize_workflow(en_workflow_hitters,
                                      best_en_hitters)

en_final_hitters <- fit(en_final_hitters, 
                        data = hitters_train)
```

These final models can now be applied on our testing data sets to assess their abilities to generalize to brand new data -- first we'll do the MLB data:

```{r}
augment(en_final_hitters, new_data = hitters_test) %>%
  rmse(truth = salary, estimate = .pred)
```

Then the customer churn data:

```{r}
en_final_churn <- finalize_workflow(en_workflow_churn,
                                      best_en_churn)

en_final_churn <- fit(en_final_churn, 
                        data = mlc_train)

augment(en_final_churn, new_data = mlc_train) %>%
  roc_auc(churn, .pred_yes)
```

#### Activities

-   Evaluate the performance of both of these models on the testing sets.
-   Does the performance of each model increase or decrease when you go from training to testing? Why do you think this might be?

## Resources

The free book [Tidy Modeling with R](https://www.tmwr.org/) is strongly recommended (as are the other resources linked in the course syllabus).
