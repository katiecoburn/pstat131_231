---
title: "Lab 4: Resampling"
author: "PSTAT 131/231"
output:
    html_document:
      toc: true
      toc_float: true
      code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

## Introduction

This lab will show how to perform different resampling techniques. These tasks are quite general and useful in many different areas. In this course, we'll primarily be using **stratified *k*-fold cross-validation**, but other techniques (like bootstrapping) can be valuable in different cases.

This chapter will involve using `rsample` for creating resampled data frames, as well as [`yardstick`](https://yardstick.tidymodels.org/) to calculate performance metrics. Lastly, we also use [`tune`](https://tune.tidymodels.org/) to fit our models within said resamples and [`dials`](https://dials.tidymodels.org/), which is used together with `tune` to select hyperparameter tuning values. All these packages are automatically loaded when we load `tidymodels`, so we don't have to worry about loading them individually.

### Loading Packages

We load `tidymodels` for modeling functions, `ISLR` and `ISLR2` for data sets, and the `tidyverse`:

```{r}
library(tidymodels)
library(ISLR)
library(ISLR2)
library(tidyverse)
library(ggthemes)
tidymodels_prefer()
```

`tidymodels_prefer()` isn't required for the packages to load, but it ensures that, for any packages that have functions with the same name, the `tidyverse` version will be prioritized (which can prevent some errors).

### Data

We'll be working with the `Auto` data set for this lab.

```{r}
Auto <- tibble(ISLR::Auto) %>% 
  mutate(origin = factor(origin))
```

Note that we're making `origin` a factor here, directly after reading in the data, because we know from the help page for the data set that `origin` is a categorical variable representing each car's country of origin. However, it's stored as a number (1, 2, or 3), so if we don't manually make it a factor variable it may be mishandled.

#### Activities

-   Access the help page for `Auto`. Familiarize yourself with the subject of the data set -- the outcome variable (`mpg`) and the predictor variables.

## The Initial Split

When fitting a model, it is often desired to be able to calculate a performance metric to quantify how well the model fits the data. If a model is evaluated on the data it was fit on, you are almost guaranteed to get overly optimistic results. Therefore, we always split our data into testing and training. This way we can fit the model to data and evaluate it on some other data that is similar.

Splitting of the data is **often** done using random sampling, so it is advised to set a seed before splitting to assure we can reproduce the results. The `initial_split()` function takes a `data.frame` and returns a `rsplit` object. This object contains information about which observations belong to which data set -- testing and training. This is where you would normally set a proportion of data that is used for training and how much is used for evaluation. This is set using the `prop` argument.

We also usually set the `strata` argument. This argument makes sure that both sides of the split have roughly the same distribution for each value of the stratifying variable. If a numeric variable is passed to `strata`, then it is binned and distributions are matched within bins. If we stratify along a discrete variable, each sample (training and testing) will have approximately the same proportion of each level of the outcome.

Even when we go on to do cross-validation, we **must** perform the initial split.

```{r}
set.seed(3435)
Auto_split <- initial_split(Auto, strata = mpg, prop = 0.7)
Auto_split
```

The testing and training data sets can be generated using the `testing()` and `training()` functions, respectively:

```{r}
Auto_train <- training(Auto_split)
Auto_test <- testing(Auto_split)
```

And if we wanted to verify, we could use `dim()` to check that the correct number of observations are now in each data set:

```{r}
dim(Auto_train)
dim(Auto_test)
```

Up until now, we have either been choosing a best-fitting model by comparing **training set** performance, or simply by comparing every model's performance on the testing set. However, neither of these methods is ideal -- the first because it *almost always* results in overfitting, the second because it essentially means we're using the testing set as a second training set.

We'll take some time now to go over the three most commonly used resampling methods, with the caveat that, in most cases, *k*-fold cross-validation will be our default and will work fairly well.

There are some specific cases -- for instance, time series data, where there is an autocorrelation between observations, etc. -- where simple random sampling into training and testing sets isn't sufficient. In those cases, to retain the correlation, we might decide to keep some days (or years, etc) as a training set and set aside another year or day's worth of data for testing. These examples aren't directly featured in this course.

## Validation Set Approach

Now that we have a train-test split, to use the validation set approach, we need to create one more random sample from the training set; this will be our **validation set**. We'll **train** our models on the training sample, **choose** a best-fitting model by comparing their. performance on the validation set, and ultimately fit our best-fitting modelto the **testing set** and **evaluate** its performance.

```{r}
Auto_valid <- validation_split(Auto_train, prop = .75, strata = mpg)
```

Before we move on, it is important to reiterate that you should **only** use the **testing** data set **once**! Once you have assessed your model's performance on the testing data set, you should not modify your models. If you do, you might end up overfitting the model due to *data leakage* -- the phenomenon that occurs when there is an overlap between training data and testing data.

Here, our modeling goal is to predict `mpg` with `horsepower`, `displacement`, `weight`, and `origin` using both a simple linear regression model and a *k*-nearest neighbors regression model with *k* of 5 (the default value). We want to determine which of these models is a better fit.

### Linear Model

First, we set up a basic model, predicting `mpg` with `horsepower`, and a linear regression specification:

```{r}
Auto_recipe <- recipe(mpg ~ horsepower + displacement +
                        weight + origin, data = Auto_train) %>% 
  step_dummy(origin)

lm_mod <- linear_reg() %>%
  set_mode("regression") %>%
  set_engine("lm")

lm_wkflow <- workflow() %>% 
  add_model(lm_mod) %>% 
  add_recipe(Auto_recipe)
```

Rather than using `fit()`, we need to use `fit_resamples()`. This is because the validation set is technically considered a resample of the training set and is stored as a resample object in `R`. So we can fit the linear model to the validation set like so:

```{r}
lm_fit_val <- lm_wkflow %>% 
  fit_resamples(resamples = Auto_valid)
```

We can now view the model results by using `collect_metrics()`:

```{r}
collect_metrics(lm_fit_val)
```

By default, for a regression problem, `collect_metrics()` gives us the RMSE and $R^2$ values. They represent the metrics for the model when fit to the resample(s). Since there is only one resample (the validation set), $n = 1$ and there is no standard error provided for the average RMSE, etc.

In our example, we get a RMSE value of 4.57 and an $R^2$ value of 0.72. These specific values will vary a bit depending on your seed, since the random sampling used in splitting the data set will be different, both for the initial split and for the validation sample.

#### Activities

-   Try fitting a simple linear regression of `mpg` on `horsepower` to the validation set. What RMSE did you obtain? Compare this to the RMSE from the multiple linear regression.

### *k*-Nearest Neighbors

Next we will fit a *k*-nearest neighbors model, setting $k = 5$.

```{r}
knn_mod <- nearest_neighbor(neighbors = 5) %>%
  set_mode("regression") %>%
  set_engine("kknn")

knn_wkflow <- workflow() %>% 
  add_model(knn_mod) %>% 
  add_recipe(Auto_recipe)
```

We can now fit this model. Again, remember to fit it on the resampled validation set, `Auto_valid`.

```{r}
knn_fit_val <- knn_wkflow %>% 
  fit_resamples(resamples = Auto_valid)
```

The RMSE on the **validation set** is then calculated as:

```{r}
collect_metrics(knn_fit_val)
```

The **validation set** RMSE for this KNN model is 3.31 and the $R^2$ value is 0.85; both of these metrics indicate that the k-nearest neighbors model with $k = 5$ is likely to perform better on new data than the linear regression model. However, remember that we selected this value of *k* arbitrarily; we do not know whether a different value, like $k = 3$, for example, might do even better. We also only have one estimate of these models' performance on new data, because we've used the **validation set approach**, which is not often the optimal resampling method.

If we were done here, and wanted to go on to use our KNN model with 5 neighbors for new data, we could assess the performance of the 5-neighbor model on the testing set, and we would obtain an RMSE of about 3.75:

```{r}
final_val_fit <- fit(knn_wkflow, Auto_train)
augment(final_val_fit, new_data = Auto_test) %>%
  rmse(truth = mpg, estimate = .pred)
```

#### Activities

-   Try fitting *k*-nearest neighbors, with *k* = 5*,* using the simpler model that predicts `mpg` with only `horsepower`. What RMSE did you obtain from the validation set for this simpler KNN model?

## *k*-fold Cross-Validation

Earlier, we set `neighbors = 5` to fit a *k*-nearest neighbors model with specifically 5 neighbors. But suppose we want to find the best value of `neighbors` that yields the best performance when applied to new data. The process of identifying the best value to set a hyperparameter to is known as hyperparameter **tuning**, and it is a case where we can use k-fold cross-validation.

To use k-fold cross-validation, we need 3 things:

-   A `workflow` object with one or more arguments marked for tuning,

-   A `vfold_cv` rsample object of the cross-validation resamples, &

-   A tibble denoting the values of hyperparameter(s) to be explored.

Here, we are tuning on just one parameter -- namely, the `neighbors` argument in `nearest_neighbors()` -- but the same process can be used to tune multiple hyperparameters. Creating a new recipe with `neighbors = tune()` indicates that we intend for the number of neighbors to be tuned.

Note that before, we set a specific value for `neighbors`; setting it to `tune()` is like flagging it as something for which we'll try out multiple candidate values.

```{r}
knn_mod_cv <- nearest_neighbor(neighbors = tune()) %>%
  set_mode("regression") %>%
  set_engine("kknn")

knn_wkflow_cv <- workflow() %>% 
  add_model(knn_mod_cv) %>% 
  add_recipe(Auto_recipe)
```

Because we've set `neighbors = tune()`, this means that we would not be able to fit this workflow right now as the value of `neighbors` is required but unspecified, and if we tried we would likely get an error.

The next thing we need to create is the k-fold data set. This can be done using the `vfold_cv()` function. Note that the function uses the term `v` instead of `k`, which is the terminology of ISLR, but the two terms are equivalent. Common choices for `v` are 5 or 10. Here, we use 10 folds.

```{r}
Auto_folds <- vfold_cv(Auto_train, v = 10)
Auto_folds
```

The result is a tibble of `vfold_splits`, which is similar to the validation split we created earlier, but now there are 10 rows -- one per fold -- and each row consists of one of the 10 data sets. We could, if we were interested, even extract the data from a single assessment set, for example:

```{r}
# This selects the first column, all the split identifications:
Auto_folds[[1]] %>% 
  head()
# And the very first one:
Auto_folds[[1]][[1]]
# And we can use testing() to see the observations:
testing(Auto_folds[[1]][[1]])
```

The raw data from one of ten assessment sets, however, is not terribly interesting or useful to us. We're more interested in the metric results from fitting different models to each of these 10 pairs of folds. But before we can get those results, we have to decide what models we'll be fitting, or, in other words, what values of *k* to test.

So the last thing we need is a tibble of possible values we want to explore. Each of the tuneable parameters in `tidymodels` has an associated function in the `dials` package. We need to use the `neighbors()` function here, and we'll extend the range to have a max of 10. The `dials` function `neighbors()` is then passed to `grid_regular()` to create a regular grid of values.

We could set the range of `neighbors()` to cover any values that we're interested in. An upper limit of 10 is a fairly arbitrary choice, but trying out these first ten values of *k* should be a good start.

```{r}
neighbors_grid <- grid_regular(neighbors(range = c(1, 10)), levels = 10)
neighbors_grid
```

Now that all the necessary objects have been created, we can pass them to `tune_grid()`, which will fit the models within each fold for each value specified in `degree_grid`.

```{r}
tune_res <- tune_grid(
  object = knn_wkflow_cv, 
  resamples = Auto_folds, 
  grid = neighbors_grid
)
```

It can be helpful to add `control = control_grid(verbose = TRUE)`; this will print out the progress, which can be especially helpful when the models take a while to fit. Note that we've set `message=TRUE` for the following code chunk so that you can see the messages this piece of code would print to your `R` console as it's being run. Setting `verbose = TRUE` prints messages to identify which fold is being worked with and which model is being run per fold.

```{r, message=TRUE}
tune_res <- tune_grid(
  object = knn_wkflow_cv, 
  resamples = Auto_folds, 
  grid = neighbors_grid,
  control = control_grid(verbose = TRUE)
)
```

`tune_res` by itself isn't easily readable, although it includes a lot of valuable information:

```{r}
tune_res
```

Luckily, the `tune` package (which again is part of `tidymodels`) provides a handful of helper functions that allow us to interpret the tuning results with relative ease!

`autoplot()` gives a visual overview of the performance of different hyperparameter values:

```{r}
autoplot(tune_res)
```

In this case, since we're tuning only one hyperparameter, this is a simple line graph -- well, two really, one for RMSE and one for $R^2$. Remember that smaller values of RMSE and larger values of $R^2$ represent better performance. It seems that using only one nearest neighbor doesn't fit very well, with $R^2$ of only 0.68, but increasing the number of neighbors tends to increase performance. However, it's worth remembering that this gain can represent overfitting (although because we're doing cross-validation, the danger of overfitting is less).

A table of results can be extracted directly with `collect_metrics()`. We also get an estimate of the standard error of the performance metrics. We get this now since we have 10 different estimates, one for each fold; we did not have standard error estimates before when we were using only one resample, the validation set.

```{r}
collect_metrics(tune_res)
```

We can also use `show_best()` to print only the top five performing models -- here, we specify `metric = "rmse"` to indicate that we want the top five models in terms of RMSE.

```{r}
show_best(tune_res, metric = "rmse")
```

Using 10 neighbors produces an RMSE of 3.92, but the difference in RMSE for 6 neighbors is only about 0.06; is it worth increasing the number of neighbors from, say, 6 to 10 for such a small potential gain in performance? How can we be sure?

There are a couple of functions in `tidymodels` that will help us automatically select models using more sophisticated rules; some of these are `select_by_one_std_err()` and `select_by_pct_loss()`. Here, we use `select_by_one_std_err()`, which selects the most simple (or parsimonious) model that is within one standard error of the numerically optimal results. We need to specify `neighbors` to tell `select_by_one_std_err()` which direction is more simple.

As a rule, we want to

-   use `desc(your_model_parameter)` if larger values of the parameter would lead to a simpler model;

-   use `your_model_parameter` if smaller values would lead to a simpler model.

The distinction is not intuitive for *k*-nearest neighbors, but let's think about the effect of changing the value of *k*; smaller values of *k* actually result in more "bendy" decision boundaries, lower bias and higher variance -- larger values of *k* are arguably more "simple" or less complex models. So we'll use `desc(neighbors)`:

```{r}
select_by_one_std_err(tune_res, desc(neighbors), metric = "rmse")
```

This selected `neighbors = 10`. We will use this value.

```{r}
best_neighbors <- select_by_one_std_err(tune_res, desc(neighbors), metric = "rmse")
```

This selected value can be now be used to specify the previous unspecified `neighbors` argument in `knn_wkflow_cv` using `finalize_workflow()`:

```{r}
final_wf <- finalize_workflow(knn_wkflow_cv, best_neighbors)

final_wf
```

This workflow can now be fitted, and we want to make sure we fit it on the full training data set. To illustrate, remember that what we had been doing was equivalent to fitting each of 10 different KNN models to each of 10 **miniature** training sets -- one per fold. Now that we've chosen a value of *k*, or one of the 10 KNN models, that we think is ideal, we'll fit **that specific** **model** to our entire training set. Doing so means we're using as much data as we can to fit the ideal model, which hopefully means it will be able to perform even better when predicting the testing set.

*(Remember that the reason we don't fit all 10 candidate KNN models to the entire training set is because doing so would not allow us to estimate their testing RMSE the way cross-validation has.)*

```{r}
final_fit <- fit(final_wf, Auto_train)

final_fit
```

Finally, we assess its RMSE on the testing set:

```{r}
augment(final_fit, new_data = Auto_test) %>%
  rmse(truth = mpg, estimate = .pred)
```

Lo and behold, the KNN model with $k = 10$ has produced a **testing RMSE** of 3.50 -- the lowest yet, even lower than the estimated testing RMSE we obtained from cross-validation (which was 3.92)!

We might be tempted to try fitting a KNN model with $k = 11$ to the testing set; who knows, doing so might achieve an even lower RMSE. But yielding to this temptation would be equivalent to using our testing set as a second training set, or -- more accurately -- as a validation set. This would be considered a form of [data leakage](https://en.wikipedia.org/wiki/Leakage_(machine_learning)) and should be avoided; if we want to try out $k = 11$, we should have gone back, added that value to our grid of candidate values, and used cross-validation to assess it. So let's practice:

#### Activities

-   Expand the grid of candidate values for *k* to include values from $k = 5$ to \$k = 20\$. Use cross-validation to determine the optimal value of k using this adjusted range. Does a different value of *k* now appear to be a better choice than $k = 10$?

-   We considered the metrics RMSE and $R^2$ in this example. Figure out how to adapt the code presented to include the metric MAE (Mean Absolute Error). Create an `autoplot()` of our models' MAE across folds.

## Bootstrap Illustration

For reasons described in lecture and in the textbook(s), we won't typically be using bootstrap resampling to assess model performance in this course due to a few disadvantages, although there is a `bootstraps()` function that operates much the same as `validation_split()` and `vfold_cv()`. If you want to try bootstrapping as an alternative to cross-validation, etc., you can [read more about it here](https://www.tmwr.org/resampling.html#bootstrap).

This is a small section of code in which we use bootstrapping as part of a mostly educational exercise to estimate the accuracy of the linear regression model on the `Auto` data set.

Suppose we want to study the **variability** of the slope and intercept estimate of a simple linear regression model predicting `mpg` with `horsepower`, and we want to do so using bootstrap resampling. How can we do this?

First, we can use the `bootstraps()` function mentioned above to create some bootstrap resamples of the training data:

```{r}
Auto_boots <- bootstraps(Auto)
```

Then we create a function that takes a split and returns some values. This function will return a tibble of parameter estimates for each bootstrap. This is likely the simplest way to accomplish what we want to do here, which is to extract the actual $\hat{\beta_0}$ and $\hat{\beta_1}$ values from the estimated linear regression for each bootstrap sample. `collect_metrics()` or similar would not work here, because that function is designed to extract a metric representing the model's overall performance, like RMSE, not to extract the individual parameter estimates.

```{r}
boot_fn <- function(split) {
  # The function's only argument is a resample object, here called 'split'
  # For each resample object, we use linear regression and the lm engine:
  lm_fit <- linear_reg() %>%
  set_engine("lm") %>% 
  # to fit our simple linear model to the resample object:
  fit(mpg ~ horsepower, data = analysis(split))
  # The function then returns the parameter estimates, standard errors, etc.
  return(tidy(lm_fit))
}
```

Then we use `mutate()` and `map()` to apply the function to each of the bootstraps. This adds a new column to our data called `models`, which will contain the parameter estimates for each bootstrap resample. You can see this by uncommenting and running the line `# boot_res`.

```{r}
boot_res <- Auto_boots %>%
  mutate(models = map(splits, boot_fn))
# boot_res
```

The use of the `map()` function is necessary because the `splits` column of `Auto_boots` is a list. You can read more about `map()` functions and their uses in one of the recommended texts from the syllabus [here](https://r4ds.had.co.nz/iteration.html?q=map#the-map-functions).

We can now `unnest()` the column of model results:

```{r}
boot_res %>% 
  unnest(models)
```

Note that this has essentially expanded the tibble from 25 rows, one per bootstrap resample, to 50 rows, **two** per bootstrap resample (one row displaying the intercept and one displaying the slope estimate). If we want to look at the **average** intercept and slope estimates across resamples, or -- as mentioned above -- if we want to assess the **standard deviations** of the intercept and slope estimates, we should use the `group_by()` and `summarise()` functions.

`group_by()` doesn't visibly change the results, but now tells us that there is one group, `term [2]` (because there are two possible values of `term`, intercept and slope):

```{r}
boot_res %>%
  unnest(models) %>%
  group_by(term)
```

`summarise()` is our powerhouse function here. It applies whatever function(s) we like -- here, `mean()` and `var()`, or mean and variance -- to each level of the grouping variable we specified in `group_by()`:

```{r}
boot_res %>%
  unnest(models) %>%
  group_by(term) %>%
  summarise(mean = mean(estimate),
            sd = sd(estimate))
```

Ta-da! We've used bootstrap resampling to obtain an estimate of $E[\hat{\beta_0}]$ and \$E[\\hat{\\beta_1}]\$, or the means of the bootstrap sampling distributions of the parameters, along with estimates of the standard deviations of these bootstrap sampling distributions.

One nice thing about bootstrap resampling is that it can be used to obtain approximate sampling distributions for virtually any statistic, even statistics whose sampling distribution is traditionally difficult to obtain. That's because the process of obtaining a bootstrap sampling distribution literally involves calculating the statistic over and over again and then assessing the results.

(However, you can imagine that doing so could potentially involve a lot of computational time and resources. We've only fit a simple linear regression here 25 times, which is not very computationally intensive at all.)

If we wanted to actually create a density curve for the bootstrapped distributions of the intercept and slope, we could:

```{r}
boot_res %>%
  unnest(models) %>%
  # rather than grouping, it's easier and
  # more concise code to filter for intercept values
  # and make each plot separately
  filter(term == "(Intercept)") %>% 
  ggplot(aes(x = estimate)) + 
  # we chose to use a density curve to make things prettier
  # a sample size of only 25 resamples yields a pretty choppy histogram
  geom_density() +
  # and let's add a line at the mean of the distribution:
  geom_vline(xintercept = 40.1722523) +
  # also let's use the ggthemes package to make things nicer-looking
  # because why not.
  theme_bw()
```

#### Activities

-   Make a density curve plot for the bootstrapped distribution of the slope. Add a line at its mean.
-   Compare the standard deviations we obtained via bootstrapping for the intercept and slope of this linear model to the standard deviations obtained by fitting one simple linear model to the entire training set (`lm(mpg ~ horsepower, data = Auto_train)`). What do you notice?

## Resources

The free book [Tidy Modeling with R](https://www.tmwr.org/) is strongly recommended.

## Source

Several parts of this lab use code from the ["ISLR Tidymodels Labs"](https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/classification.html). Credit to Emil Hvitfeldt for writing and maintaining the open-source book.
