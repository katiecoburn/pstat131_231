---
title: "Lab 4: Resampling"
author: "PSTAT 131/231"
output:
    html_document:
      toc: true
      toc_float: true
      code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

## Introduction

This lab will show how to perform different resampling techniques. Some of these tasks are quite general and useful in many different areas. In this course, we'll primarily be using stratified *k*-fold cross-validation, but other techniques can be valuable in different cases.

This chapter will bring [`rsample`](https://www.tidymodels.org/start/resampling/) into view for creating resampled data frames as well as [`yardstick`](https://yardstick.tidymodels.org/) to calculate performance metrics. Lastly, we will also use [`tune`](https://tune.tidymodels.org/) to fit out models within said resamples. We also see a use of [`dials`](https://dials.tidymodels.org/), which are used together with `tune` to select hyperparameter tuning values. All these packages are loaded when we load `tidymodels`.

### Loading Packages

We load `tidymodels` for modeling functions, `ISLR` and `ISLR2` for data sets, and the `tidyverse`.

```{r}
library(tidymodels)
library(ISLR)
library(ISLR2)
library(tidyverse)
tidymodels_prefer()
```

### Data

We'll be working with the `Auto` and `Carseats` data sets for this lab.

```{r}
Auto <- tibble(ISLR::Auto)
Carseats <- tibble(Carseats)
```

#### Activities

- Access the help page for both `Auto` and `Portfolio`. Familiarize yourself with the subject of each data set and the predictor variables.

## The Initial Split

When fitting a model, it is often desired to be able to calculate a performance metric to quantify how well the model fits the data. If a model is evaluated on the data it was fit on, you are quite likely to get over-optimistic results. Therefore, we always split our data into testing and training. This way we can fit the model to data and evaluate it on some other data that is similar.

Splitting of the data is done using random sampling, so it is advised to set a seed before splitting to assure we can reproduce the results. The `initial_split()` function takes a `data.frame` and returns a `rsplit` object. This object contains information about which observations belong to which data set, testing, and training. This is where you would normally set a proportion of data that is used for training and how much is used for evaluation. This is set using the `prop` argument. We also set the `strata` argument. This argument makes sure that both sides of the split have roughly the same distribution for each value of strata. If a numeric variable is passed to `strata`, then it is binned and distributions are matched within bins.

Even when doing another approach such as cross-validation, you **must** perform the initial split. Fitting models to the entire training set would then be a validation set approach, while cross-validation would constitute resampling from the training set.

```{r}
set.seed(3435)
Auto_split <- initial_split(Auto, strata = mpg, prop = 0.7)
Auto_split
```

The testing and training data sets can be materialized using the `testing()` and `training()` functions, respectively:

```{r}
Auto_train <- training(Auto_split)
Auto_test <- testing(Auto_split)
```

We can use `dim()` to verify that the correct number of observations are now in each data set:

```{r}
dim(Auto_train)
dim(Auto_test)
```

#### Activities

- Split the `Carseats` data into training and testing sets, with 80\% in training and 20\% in testing. Stratify the split on `Sales`.

- Verify that each set has the correct number of observations.

## Validation Set Approach

Now that we have a train-test split, to use the validation set approach, let us fit some models **to the entire training set** and evaluate their performance. *Note that this is the approach we've been using on the homework and lab assignments thus far.*

Before we move on, it is important to reiterate that you should only use the testing data set **once**! Once you have looked at the performance on the testing data set, you should not modify your models. If you do, you might overfit the model due to data leakage.

Our modeling goal is to predict `mpg` by `horsepower` using a simple linear regression model and a polynomial regression model.

### Linear Model

First, we set up a linear regression specification:

```{r}
lm_spec <- linear_reg() %>%
  set_mode("regression") %>%
  set_engine("lm")
```

And we fit it like normal. Note that we are fitting it using Auto_train.

```{r}
lm_fit <- lm_spec %>% 
  fit(mpg ~ horsepower, data = Auto_train)
```

We can now use the `augment()` function to extract the predictions and `rmse()` to calculate the root mean squared error. This will be the testing RMSE, since we are evaluating on `Auto_test`.

```{r}
augment(lm_fit, new_data = Auto_test) %>%
  rmse(truth = mpg, estimate = .pred)
```

and we get a RMSE of 4.77. This specific value is going to vary a little depending on what seed number you picked, since the random sampling used in splitting the data set will be slightly different.

Using this framework makes it easy for us to calculate the training RMSE:

```{r}
augment(lm_fit, new_data = Auto_train) %>%
  rmse(truth = mpg, estimate = .pred)
```

Comparing these two values can give us a look into how generalizable the model is to data it hasn’t seen before. We do expect the training RMSE to be lower than the testing RMSE, but if you see a large difference, there is an indication of overfitting or a shift between the training data set and testing data set.

#### Activities

- Fit a linear regression to the `Carseats` data, predicting `Sales`.

- Calculate the training and testing RMSE for the `Carseats` regression.

### Polynomial Regression

Next we will fit a polynomial regression model. We can use the linear model specification `lm_spec` to add a preprocessing unit with `recipe()` and `step_poly()` to create the polynomial expansion of `horsepower`. We can combine these two with `workflow()` to create a workflow object.

```{r}
poly_rec <- recipe(mpg ~ horsepower, data = Auto_train) %>%
  step_poly(horsepower, degree = 2)

poly_wf <- workflow() %>%
  add_recipe(poly_rec) %>%
  add_model(lm_spec)

poly_wf
```

We can now fit this model. Again, remember to fit it on the training data set, `Auto_train`.

```{r}
poly_fit <- fit(poly_wf, data = Auto_train)
```

The testing RMSE is then calculated as:

```{r}
augment(poly_fit, new_data = Auto_test) %>%
  rmse(truth = mpg, estimate = .pred)
```

Which is a bit lower. So it would appear, from this, that the polynomial regression model has a better fit.

#### Activities

- Fit a polynomial regression to the `Carseats` data, predicting `Sales`.

- Calculate the training and testing RMSE for the `Carseats` regression.

## *k*-fold Cross-Validation

Earlier, we set `degree = 2` to create a second-degree polynomial regression model. But suppose we want to find the best value of degree that yields the “closest” fit. This is known as hyperparameter tuning, and it is a case where we can use k-Fold Cross-Validation. To use k-Fold Cross-Validation, we will be using the `tune` package, and we need 3 things to get it working:

- A parsnip/workflow object with one or more arguments marked for tuning,

- A `vfold_cv` rsample object of the cross-validation resamples, &

- A tibble denoting the values of hyperparameter values to be explored.

Here, we are doing the hyperparameter tuning on just one parameter, namely the `degree` argument in `step_poly()`, but the same process can be used to tune multiple hyperparameters. Creating a new recipe with `degree = tune()` indicates that we intend for degree to be tuned.

```{r}
poly_tuned_rec <- recipe(mpg ~ horsepower, data = Auto_train) %>%
  step_poly(horsepower, degree = tune())

poly_tuned_wf <- workflow() %>%
  add_recipe(poly_tuned_rec) %>%
  add_model(lm_spec)
```

This means that would not be able to fit this workflow right now as the value of degree is unspecified, and if we try we get an error:

```{r, error=TRUE}
fit(poly_tuned_wf, data = Auto_train)
```

The next thing we need to create is the k-Fold data set. This can be done using the `vfold_cv()` function. Note that the function uses `v` instead of `k`, which is the terminology of ISLR. Common choices for k/v are 5 or 10. Here, we use 10 folds.

```{r}
Auto_folds <- vfold_cv(Auto_train, v = 10)
Auto_folds
```

The result is a tibble of `vfold_splits` which is quite similar to the `rsplit` object we saw earlier.

The last thing we need is a tibble of possible values we want to explore. Each of the tuneable parameters in `tidymodels` has an associated function in the `dials` package. We need to use the `degree()` function here, and we extend the range to have a max of 10. This `dials` function is then passed to `grid_regular()` to create a regular grid of values.

```{r}
degree_grid <- grid_regular(degree(range = c(1, 10)), levels = 10)
degree_grid
```

Using `grid_regular()` is a little overkill for this application, with only one parameter, but once you have multiple parameters you want to tune, the `grid` functions are very helpful.

Now that all the necessary objects have been created, we can pass them to `tune_grid()`, which will fit the models within each fold for each value specified in `degree_grid`.

```{r}
tune_res <- tune_grid(
  object = poly_tuned_wf, 
  resamples = Auto_folds, 
  grid = degree_grid
)
```

It can be helpful to add `control = control_grid(verbose = TRUE)`; this will print out the progress, which can be especially helpful when the models take a while to fit. `tune_res` by itself isn’t easily readable. Luckily, `tune` provides a handful of helper functions.

`autoplot()` gives a visual overview of the performance of different hyperparameter pairs:

```{r}
autoplot(tune_res)
```

It appears that the biggest jump in performance comes from going to `degree = 2`. Afterward, there might be a little bit of improvement, but it isn’t as obvious.

The number used for plotting can be extracted directly with `collect_metrics()`. We also get an estimate of the standard error of the performance metric. We get this since we have 10 different estimates, one for each fold.

```{r}
collect_metrics(tune_res)
```

You can also use `show_best()` to only show the best performing models:

```{r}
show_best(tune_res, metric = "rmse")
```

We did see that the performance plateaued after `degree = 2`. There are a couple of functions to select models by more sophisticated rules, `select_by_one_std_err()` and `select_by_pct_loss()`. Here, we use `select_by_one_std_err()` which selects the most simple model that is within one standard error of the numerically optimal results. We need to specify `degree` to tell `select_by_one_std_err()` which direction is more simple.

You want to

- use desc(you_model_parameter) if larger values lead to a simpler model;

- use you_model_parameter if smaller values lead to a simpler model.

Lower polynomials models are simpler, so we ditch `desc()`.

```{r}
select_by_one_std_err(tune_res, degree, metric = "rmse")
```

This selected `degree = 2`. We will use this value, since using simpler models sometimes can be very beneficial -- especially if we want to explain what happens in it.

```{r}
best_degree <- select_by_one_std_err(tune_res, degree, metric = "rmse")
```

This selected value can be now be used to specify the previous unspecified `degree` argument in `poly_wf` using `finalize_workflow()`.

```{r}
final_wf <- finalize_workflow(poly_wf, best_degree)

final_wf
```

This workflow can now be fitted. And we want to make sure we fit it on the full training data set.

```{r}
final_fit <- fit(final_wf, Auto_train)

final_fit
```

Finally, we assess its accuracy on the testing set:

```{r}
augment(final_fit, new_data = Auto_test) %>%
  rmse(truth = mpg, estimate = .pred)
```

#### Activities

- Repeat all these steps on the `Carseats` data.

- Which `degree` value results in the best-fitting model for `Carseats`?

## Bootstrap

This section illustrates the use of the bootstrap in the simple Section 5.2 of ISLR, as well as on an example involving estimating the accuracy of the linear regression model on the `Auto` data set.

In this example, we want to study the variability of the slope and intercept estimate of the linear regression model. First, we create some bootstraps of the data. Then we create a function that takes a split and returns some values. This function will return a tibble for each bootstrap.

```{r}
Auto_boots <- bootstraps(Auto)

boot.fn <- function(split) {
  lm_fit <- lm_spec %>% fit(mpg ~ horsepower, data = analysis(split))
  tidy(lm_fit)
}
```

Then we use `mutate()` and `map()` to apply the function to each of the bootstraps:

```{r}
boot_res <- Auto_boots %>%
  mutate(models = map(splits, boot.fn))
```

And we can now `unnest()` and use `group_by()` and `summarise()` to get an estimate of the variability of the slope and intercept in this linear regression model.

```{r}
boot_res %>%
  unnest(cols = c(models)) %>%
  group_by(term) %>%
  summarise(mean = mean(estimate),
            sd = sd(estimate))
```

#### Activities

- Repeat this on the `Carseats` data.

## Resources

The free book [Tidy Modeling with R](https://www.tmwr.org/) is strongly recommended.

## Source

Several parts of this lab come directly from the ["ISLR Tidymodels Labs"](https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/classification.html). Credit to Emil Hvitfeldt for writing and maintaining the open-source book.