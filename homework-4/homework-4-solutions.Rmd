---
title: "Homework 4"
author: "PSTAT 131/231"
output:
    html_document:
      toc: true
      toc_float: true
      code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

## Resampling

For this assignment, we will continue working with part of a [Kaggle data set](https://www.kaggle.com/c/titanic/overview) that was the subject of a machine learning competition and is often used for practicing ML models. The goal is classification; specifically, to predict which passengers would survive the [Titanic shipwreck](https://en.wikipedia.org/wiki/Titanic).

![Fig. 1: RMS Titanic departing Southampton on April 10, 1912.](images/RMS_Titanic.jpg){width="363"}

Load the data from `data/titanic.csv` into *R* and familiarize yourself with the variables it contains using the codebook (`data/titanic_codebook.txt`).

Notice that `survived` and `pclass` should be changed to factors. When changing `survived` to a factor, you may want to reorder the factor so that *"Yes"* is the first level.

Make sure you load the `tidyverse` and `tidymodels`!

*Remember that you'll need to set a seed at the beginning of the document to reproduce your results.*

Create a recipe for this dataset **identical** to the recipe you used in Homework 3.

```{r}
library(ISLR)
library(ISLR2)
library(tidyverse)
library(tidymodels)
library(readr)
library(corrr)
library(corrplot)
library(discrim)
library(klaR)
library(tune)
tidymodels_prefer()

titanic <- read.csv("data/titanic.csv") %>% 
  mutate(survived = factor(survived, levels = c("Yes", "No")))
```


### Question 1

Split the data, stratifying on the outcome variable, `survived.`  You should choose the proportions to split the data into. Verify that the training and testing data sets have the appropriate number of observations. 

```{r}
set.seed(3435)

titanic_split <- titanic %>% 
  initial_split(strata = survived, prop = 0.7)
titanic_train <- training(titanic_split)
titanic_test <- testing(titanic_split)
dim(titanic_train)
dim(titanic_test)


control <- control_resamples(save_pred = TRUE)

titanic_recipe <- recipe(survived ~ pclass + sex + age + 
                                 sib_sp + parch + fare, data = titanic_train) %>%
  step_impute_linear(age, impute_with = imp_vars(sib_sp)) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(~ starts_with("sex"):fare + age:fare)
```

*Most proportions are acceptable here as long as it's greater than $0.50$, so more observations are in training than testing, and less than $0.99$, so there are at least some observations in testing.* 

### Question 2

Fold the **training** data. Use *k*-fold cross-validation, with $k = 10$.

```{r}
titanic_folds <- vfold_cv(titanic_train, strata = survived, 
                          v = 10)
```

*Stratifying in the cross-validation is not required, but often a good idea, especially if there may be a level imbalance.*

### Question 3

In your own words, explain what we are doing in Question 2. What is *k*-fold cross-validation? Why should we use it, rather than simply fitting and testing models on the entire training set? If we **did** use the entire training set, what resampling method would that be?

*Answers may vary. The key ideas are that k-fold cross-validation is a resampling method in which the data are randomly divided into k folds and each one is individually treated as a miniature testing set, while the other folds are used for training, respectively. It is better than using the entire training set for fitting and testing because we cannot get a valid idea of model performance on new data if we test them on the data they were trained with.*

*Note: There is a bit of a typo in the last question; it's meant to say "If we used part of the training set for validation." The correct answer without a typo is the validation set method, but any answer here is fine, since there was a typo.*

### Question 4

Set up workflows for 3 models:

1. A logistic regression with the `glm` engine;
2. A linear discriminant analysis with the `MASS` engine;
3. A quadratic discriminant analysis with the `MASS` engine.

```{r}
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

log_wkflow <- workflow() %>% 
  add_recipe(titanic_recipe) %>% 
  add_model(log_reg)

lda_mod <- discrim_linear() %>% 
  set_engine("MASS") %>% 
  set_mode("classification")

lda_wkflow <- workflow() %>% 
  add_recipe(titanic_recipe) %>% 
  add_model(lda_mod)

qda_mod <- discrim_quad() %>% 
  set_engine("MASS") %>% 
  set_mode("classification")

qda_wkflow <- workflow() %>% 
  add_recipe(titanic_recipe) %>% 
  add_model(qda_mod)
```

How many models, total, across all folds, will you be fitting to the data? To answer, think about how many folds there are, and how many models you'll fit to each fold.

*We are fitting three types of models to each of 10 folds, for a total of 30 models.*

### Question 5

Fit each of the models created in Question 4 to the folded data.

**IMPORTANT:** *Some models may take a while to run – anywhere from 3 to 10 minutes. You should NOT re-run these models each time you knit. Instead, run them once, using an R script, and store your results; look into the use of [loading and saving](https://www.r-bloggers.com/2017/04/load-save-and-rda-files/). You should still include the code to run them when you knit, but set `eval = FALSE` in the code chunks.*

```{r}
log_fit <- fit_resamples(log_wkflow, titanic_folds)

lda_fit <- fit_resamples(resamples = titanic_folds, 
                         lda_wkflow,  
                         control = control)

qda_fit <- fit_resamples(qda_wkflow, resamples = titanic_folds,
                         control = control)
```

### Question 6

Use `collect_metrics()` to print the mean and standard errors of the performance metric *accuracy* across all folds for each of the four models.

Decide which of the 3 fitted models has performed the best. Explain why. *(Note: You should consider both the mean accuracy and its standard error.)*

```{r}
collect_metrics(log_fit)
collect_metrics(lda_fit)
collect_metrics(qda_fit)
```

*Various answers here are fine, as long as they consider both the mean accuracy and its standard error. For example, in this case, an argument could be made for the QDA model which is almost one standard error away from logistic regression, or for the logistic regression because it is within one standard error of the other two and a simpler model.*

### Question 7

Now that you’ve chosen a model, fit your chosen model to the entire training dataset (not to the folds).

```{r}
log_fit_train <- fit(log_wkflow, titanic_train)
```

### Question 8

Finally, with your fitted model, use `predict()`, `bind_cols()`, and `accuracy()` to assess your model’s performance on the testing data!

Compare your model’s testing accuracy to its average accuracy across folds. Describe what you see.

```{r}
log_test <- fit(log_wkflow, titanic_test)
predict(log_test, new_data = titanic_test, type = "class") %>% 
  bind_cols(titanic_test %>% select(survived)) %>% 
  accuracy(truth = survived, estimate = .pred_class)
```

*For logistic regression, the model actually performed better in terms of accuracy on the testing set than it did on the training set. This can happen sometimes, and is likely because this is not a difficult prediction problem -- and, in this case, possibly because the assumptions of logistic regression are met.*

## Required for 231 Students

Consider the following intercept-only model, with $\epsilon \sim N(0, \sigma^2)$:

$$
Y=\beta+\epsilon
$$

where $\beta$ is the parameter that we want to estimate. Suppose that we have $n$ observations of the response, i.e. $y_{1}, ..., y_{n}$, with uncorrelated errors.

### Question 9

Derive the least-squares estimate of $\beta$.

$$
Y = \beta + \epsilon
$$

$$
E[Y] = E[\beta + \epsilon]
$$

$$
E[Y] = E[\beta] + E[\epsilon]
$$

$$
E[Y] = \beta+0
$$
$$
\hat{\beta}=\bar{Y}
$$

### Question 10

Suppose that we perform leave-one-out cross-validation (LOOCV). Recall that, in LOOCV, we divide the data into $n$ folds. What is the covariance between $\hat{\beta}^{(1)}$, or the least-squares estimator of $\beta$ that we obtain by taking the first fold as a training set, and $\hat{\beta}^{(2)}$, the least-squares estimator of $\beta$ that we obtain by taking the second fold as a training set?

*In order to find the least-squares estimate of $\beta$, we want to minimize $\sum_{i=1}^{n}(y_{i}-\hat{\beta})^2$.*

*The first-order condition with respect to $\hat{\beta}$ is:*

$$
-2\sum_{i=1}^{n}(y_{i}-\hat{\beta})=0
$$

$$
\sum_{i=1}^{n}y_{i}=n\hat{\beta}
$$

$$
n\bar{y}=n\hat{\beta}
$$

$$
\hat{\beta}_{OLS}=\bar{y}
$$

*We know that $\hat{\beta}^{(1)} = \frac{1}{n-1}\sum_{i\neq1}^{n}y_{i}$ and $\hat{\beta}^{(2)} = \frac{1}{n-1}\sum_{i\neq2}^{n}y_{i}$. So the covariance formula is:*

$$
Cov(\hat{\beta}^{(1)} \hat{\beta}^{(2)}) = Cov(\frac{1}{n-1}\sum_{i\neq1}^{n}y_{i} \frac{1}{n-1}\sum_{i\neq2}^{n}y_{i})
$$

$$
=\frac{1}{(n-1)^2}[Var(y_3)+...+Var(y_n)]
$$

$$
=\frac{1}{(n-1)^2}[Var(\epsilon_3)+...+Var(\epsilon_n)]
$$

$$
=\frac{n-2}{(n-1)^2}\sigma^2
$$