---
title: "Lab 6: Beyond Linearity"
author: "PSTAT 131/231"
output:
    html_document:
      toc: true
      toc_float: true
      code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

## Introduction

This lab will look at the various ways we can introduce non-linearity into our model by doing preprocessing. Methods include polynomials expansion, step functions, and splines.

This chapter will use ['parsnip'](https://www.tidymodels.org/start/models/) for model fitting and ['recipes and workflows'](https://www.tidymodels.org/start/recipes/) to perform the transformations.

We will be using the `Wage` data set from the `ISLR` package.

### Loading Packages

We load `tidymodels` for modeling functions, `ISLR` for data sets, and the `tidyverse`. We also will use the `glmnet` package to perform ridge regression.

```{r}
library(tidymodels)
library(ISLR)
library(ISLR2)
library(tidyverse)
library(glmnet)
library(modeldata)
library(ggthemes)
library(janitor) # for naming conventions
library(naniar) # to assess missing data patterns
library(corrplot) # for a correlation plot
library(patchwork) # for putting plots together
library(rpart.plot)
tidymodels_prefer()
```

### Data

We'll be working with the `Wage` data set for this lab. It consists of wage information and some data on a number of other variables for a sample size of approximately 3,000 male workers in the mid-Atlantic region.

```{r}
wage <- as_tibble(Wage) %>% 
  clean_names()
```

We can check the data quickly to see if there is any missingness, but there is none, so we won't print the plot here because it doesn't really convey any information. However, feel free to uncomment the following line and run it to verify.

```{r}
# vis_miss(wage)
```

Here is a correlation matrix of `age`, `wage`, and `year`:

```{r}
wage %>% 
  select(is.numeric, -logwage) %>% 
  cor() %>% 
  corrplot(type = "lower", diag = FALSE)
```

It's also worth taking a look at the relationships between some of the categorical predictor variables and the outcome, and at the distributions of the outcome and the continuous predictor `age`:

```{r}
p1 <- wage %>% ggplot(aes(y = education, fill = maritl,
                          x = wage)) +
  stat_summary(fun = "mean", geom = "bar", 
               position = "dodge") + theme_bw()
p2 <- wage %>% ggplot(aes(x = health_ins, fill = jobclass)) +
  geom_bar(position = "fill") + theme_bw()
p3 <- wage %>% ggplot() +
  geom_histogram(aes(x = wage), fill = "red") + theme_bw()
p4 <- wage %>% ggplot() +
  geom_histogram(aes(x = age), fill = "blue") + theme_bw()
p1 + p2 + p3 + p4 +
  plot_layout(ncol = 2)
```

#### Activities

-   Access the help page for `Wage`. Familiarize yourself with the predictor variables that are included.
-   Do you think the variable `region` will be useful in this data set? Why or why not? (Looking at the distribution of the variable might help in answering this question.)
-   What patterns or relationships do you notice between marital status, education level, and average wage?
-   Is the proportion of male employees with health insurance the same across levels of job class?

#### The Initial Split

We can start, as normal, by splitting the data into training and testing sets, using stratified sampling.

```{r}
set.seed(3435)
wage_split <- initial_split(wage, strata = "wage")

wage_train <- training(wage_split)
wage_test <- testing(wage_split)

wage_folds <- vfold_cv(wage_train, v = 5, strata = "wage")
```

## Polynomial Regression Tuning

Polynomial regression can be thought of as doing polynomial expansion on a variable and passing that expansion into a linear regression model. `step_poly()` allows us to do a polynomial expansion on one or more variables.

We'll start out with the `age` predictor and attempt to tune the value of degree in order to determine its optimal value, from 1 to 10. Notice that we can flag this argument for tuning even though it is part of the recipe; you can also tune certain parameters by flagging them in the recipe.

```{r}
rec_poly <- recipe(wage ~ age, data = wage_train) %>%
  step_poly(age, degree = tune())
```

This recipe is combined with a linear regression specification (since we're dealing with a continuous outcome, which is workers' raw wage) and then used to create a workflow object. We also set up a grid of degree values to try out, using `grid_regular()`:

```{r}
lm_spec <- linear_reg() %>%
  set_mode("regression") %>%
  set_engine("lm")

poly_wf <- workflow() %>%
  add_model(lm_spec) %>%
  add_recipe(rec_poly)

degree_grid <- grid_regular(degree(range = c(1, 10)), 
                            levels = 10)
```

We can use `tune_grid()` to fit these models to the folds:

```{r}
poly_tune <- tune_grid(poly_wf, resamples = wage_folds, 
                       grid = degree_grid)
autoplot(poly_tune)
```

One thing you may notice right away from the `autoplot()` results is that none of these models does very well when it comes to predicting `wage`. RMSE and $R^2$ both see their biggest gains in terms of performance as we go from a degree one to a degree two polynomial, and although there are some small fluctuations beyond that, degree two might be optimal here in terms of parsimony. In addition, the $R^2$ value never goes beyond 0.10 (if that), meaning that age alone explains less than 10% of variation in employee wage.

What have we essentially fit here? We've fit:

$$
Wage=\beta_0+f(Age)
$$

where

$$
f(Age)=\beta_1Age+...\beta_dAge^d
$$

for values of $d = 1,â€¦,10$ (and we've fit each of these 10 models once per fold, for a total of 50 polynomial regressions).

Another common approach is to include multiple continuous variables in `step_poly()` and specify `degree = tune()`, which then results in tuning the degree for each of these polynomial expansions simultaneously and picking the optimal degree to use for all the selected predictors.

We'll select the best model using `select_best()`, then fit our model to the training set and calculate its performance on the testing set.

```{r}
best_degree <- select_best(poly_tune)

poly_final <- finalize_workflow(poly_wf, best_degree)

poly_final_fit <- fit(poly_final, data = wage_train)
```

Note that we define a metric set here so we can call both the RMSE and $R^2$ functions on the testing set with only one line of code. This isn't technically necessary but can help make things neater.

```{r}
wage_metrics <- metric_set(rmse, rsq)

augment(poly_final_fit, new_data = wage_test) %>%
  wage_metrics(wage, .pred)
```

### Activities

-   Compare the RMSE and $R^2$ values from the folds to those for the testing set. How did the model do on brand new data?

-   What do you think would happen to the testing RMSE and $R^2$ if we used a model of degree 10 rather than degree 3? Why?

## Pruned Decision Trees

In this section, we will use the same `Wage` data set to fit and prune a decision tree. This is a regression tree because our outcome is continuous, but virtually the same exact code would apply for a classification tree; the only lines that would change would be `set_mode("regression")`, which would become `set_mode("classification")`, and `rmse(wage, .pred)`, which would become a call to whatever classification metric the user chose (i.e. ROC AUC, accuracy, true or false positive rate, etc.).

First we set up a recipe. Here, we'll remove `region`, which is a constant in this data set, and `logwage`, which is a function of `wage` itself and therefore should not be used as a predictor. We'll also dummy code where appropriate and normalize all predictors:

```{r}
rec_tree <- recipe(wage ~ ., data = wage_train) %>%
  step_rm(region, logwage) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_predictors())
```

Then we'll set up a decision tree model for regression, using the `rpart` engine, and specify that we want to tune the cost-complexity parameter (`cost_complexity = tune()`). The choice to use the `rpart` engine here is primarily motivated by wanting to use an `rpart` plotting function to visualize the pruned tree later. There are other engines to choose from; you can run `?decision_tree` to see other options. Note that most model functions in `tidymodels`, like `decision_tree()`, `logistic_reg()`, etc., all have multiple choices of engines. The engine is the package that underlies the `tidymodels` framework -- literally analogous to the engine of a car.

We'll also make a workflow and add the model and recipe:

```{r}
tree_spec <- decision_tree(cost_complexity = tune()) %>%
  set_engine("rpart") %>% 
  set_mode("regression")

tree_wf <- workflow() %>% 
  add_model(tree_spec) %>% 
  add_recipe(rec_tree)
```

We set up a grid of possible values to consider for the cost-complexity parameter and tune the models, fitting all 10 of them to each of the 5 folds for a total of 50 decision trees. Note that `cost_complexity()` also uses the `log10_trans()` function by default, so the values -3 and -1 are in the log-10 scale.

```{r}
param_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)

tune_tree <- tune_grid(
  tree_wf, 
  resamples = wage_folds, 
  grid = param_grid
)
```

Then we can visualize the results:

```{r}
autoplot(tune_tree)
```

Note that the decision tree model, with pruning, still doesn't do amazing but has done considerably better than our polynomial regression (although in that model we only included the expansion of one predictor, age).

We'll use `select_best()`, `finalize_workflow()`, and `fit()` to choose the best value for the complexity parameter and fit the "winning" model to the entire training set:

```{r}
best_complexity <- select_best(tune_tree)

tree_final <- finalize_workflow(tree_wf, best_complexity)

tree_final_fit <- fit(tree_final, data = wage_train)
```

And we can extract the model fit results and view them with the `rpart.plot()` function -- note that this wouldn't work if we'd used a different engine, although we could have used an alternative plotting function in that case.

```{r}
tree_final_fit %>%
  extract_fit_engine() %>%
  rpart.plot()
```

This plot is hard to read, however, because even with pruning the tree, the optimal value of `cost_complexity()` actually resulted in a somewhat deep tree (nowhere near as deep as they can possibly go, but deep enough to be hard to read the figure). We can view the actual splits themselves:

```{r}
tree_final_fit %>% 
  extract_fit_engine()
```

The first split the tree makes is between 2 and 3; that is, does an individual have an advanced degree?

How do we know this? Well, `education_X5..Advanced.Degree` is a dummy-coded variable generated from `education`, which takes on the values 0 or 1, 0 if someone does not have an advanced degree and 1 if they do. 2) here says that if someone has a score of \< 0.5 on that variable -- meaning if they **do not** have an advanced degree -- they will begin going down one branch of the tree, and 3) says that if they **do** have an advanced degree (a score $\geq 0.5$) they will go down the other branch.

That makes intuitive sense, or at least we hope it's true -- ideally, achieving an advanced degree would contribute to your likelihood of earning a higher wage.

Finally, let's see what the RMSE and $R^2$ of this model are on the testing data:

```{r}
augment(tree_final_fit, new_data = wage_test) %>%
  wage_metrics(wage, .pred)
```

The pruned decision tree, with a recipe that includes more predictors, has done considerably better on the testing data; its RMSE has decreased by about 6 (RMSE is in the scale of the outcome) and its $R^2$ has gone from barely 7% to about 31%.

This model is still not fantastic, but it now explains approximately 31% of the variance in `age`. A random forest or boosted tree might do even better; we'll explore these possibilities in Lab 7.

### Activities

-   Try running the same code but **without** removing `logwage`. What happens? Why does it happen? Does it make sense to use `logwage` to predict `wage`?

-   Explain why we need or want to prune decision trees at all. What tends to happen if we don't prune them?

-   What do you think one of the more useful predictors is in this model? What makes you say so?

## Resources

The free book [Tidy Modeling with R](https://www.tmwr.org/) is strongly recommended.
