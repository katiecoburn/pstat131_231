---
title: "Lab 2"
author: "PSTAT 131/231"
output:
    html_document:
      toc: true
      toc_float: true
      code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

## Introduction: Model Fitting

```{r}
library(tidyverse)
library(tidymodels)
library(ggplot2)
library(corrplot)
library(ggthemes)
library(kableExtra)
tidymodels_prefer()

set.seed(3435)

diamonds_split <- initial_split(diamonds, prop = 0.80,
                                strata = price)
diamonds_train <- training(diamonds_split)
diamonds_test <- testing(diamonds_split)
```

### Creating a Recipe

You'll notice that the textbooks use the `lm()` function to fit a linear regression. The `tidymodels` framework, however, has its own structure and flow, which is designed to work with multiple different machine learning models and packages seamlessly.

To fit any model with `tidymodels`, the first step is to create a recipe. The structure of this recipe is similar to that of `lm()`; the outcome is listed first, then the features are added:

```{r}
simple_diamonds_recipe <-
  recipe(price ~ ., data = diamonds_train)
```

Note that `.` is a placeholder for "all other variables." If we call the recipe object now, we can see some information:

```{r}
simple_diamonds_recipe
```

More specifically, we see that there are 9 predictors.

We should dummy-code all categorical predictors. We can do that easily with `step` functions:

```{r}
diamonds_recipe <- recipe(price ~ ., data = diamonds_train) %>% 
  step_dummy(all_nominal_predictors())
```

Note that we haven't specified what type of model we'll be fitting yet. The other beauty of the recipe is that it can then be directly given to one of many machine learning model "engines."

Running the above code is essentially like writing down the instructions for a recipe on a sheet of paper. We've prepared the recipe to give to the workflow, but we are probably interested in seeing what the results of the recipe itself actually look like. Did the dummy coding work, for example? To apply the recipe to a data set and view the results, we can use `prep()`, which is akin to setting out a *mise en place* of ingredients, and `bake()`.

We'll also use `kbl()` and `kable_styling()` from the [`kableExtra` package](https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html), which we installed above. It's not necessary to use these functions, but doing so allows the table of data to display more neatly, so that all the columns and rows are actually legible. We also use `head()` to select only the first few rows; otherwise the entire data frame would print, which would be time-consuming. Also note the use of `scroll_box()`, allowing us to scroll through the entire data set.

```{r}
prep(diamonds_recipe) %>% 
  bake(new_data = diamonds_train) %>% 
  head() %>% 
  kable() %>% 
  kable_styling(full_width = F) %>% 
  scroll_box(width = "100%", height = "200px")
```

#### Activities:

-   Use the Internet to find documentation about the possible `step` functions. Name three `step` functions that weren't used here and describe what they do.

## Linear Regression

Next, we can specify the model engine that we want to fit:

```{r}
lm_model <- linear_reg() %>% 
  set_engine("lm")
```

We set up a workflow. This step might seem unnecessary now, with only one model and one recipe, but it can make life easier when you are trying out a series of models or several different recipes later on.

```{r}
lm_wflow <- workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(diamonds_recipe)
```

Finally, we can fit the linear model to the training set:

```{r}
lm_fit <- fit(lm_wflow, diamonds_train)
```

We can view the model results:

```{r}
lm_fit %>% 
  # This returns the parsnip object:
  extract_fit_parsnip() %>% 
  # Now tidy the linear model object:
  tidy()
```

#### Activities:

-   Explain what the intercept represents.

-   Describe the effect of `carat`. Is it a significant predictor of `price`? Holding everything else constant, what is the effect on `price` of a one-unit increase in `carat`?

The following code generates predicted values for `price` for each observation in the training set:

```{r}
diamond_train_res <- predict(lm_fit, new_data = diamonds_train %>% select(-price))
diamond_train_res %>% 
  head()
```

Now we attach a column with the actual observed `price` observations:

```{r}
diamond_train_res <- bind_cols(diamond_train_res, diamonds_train %>% select(price))
diamond_train_res %>% 
  head()
```

We might be interested in a plot of predicted values vs. actual values, here for the training data:

```{r}
diamond_train_res %>% 
  ggplot(aes(x = .pred, y = price)) +
  geom_point(alpha = 0.2) +
  geom_abline(lty = 2) + 
  theme_bw() +
  coord_obs_pred()
```

It's fairly clear that the model didn't do very well. If it predicted every observation accurately, the dots would form a straight line. We also have predicted some negative values for price, and once the actual price is approximately over $\$5,000$, the model does a pretty poor job.

The odds are that a linear model is simply not the best tool for this machine learning task. It is likely not an accurate representation of `f()`; remember that by using a linear regression, we are imposing a specific form on the function, rather than learning the function from the data.

In future labs, we'll try out different models and compare them. Finally, we can calculate the **training** root mean squared error (RMSE) and the **testing** RMSE.

```{r}
diamond_test_res <- predict(lm_fit, new_data = diamonds_test %>% select(-price))
diamond_test_res <- bind_cols(diamond_test_res, diamonds_test %>% select(price))
  
rmse(diamond_train_res, truth = price, estimate = .pred)
rmse(diamond_test_res, truth = price, estimate = .pred)
```

We can create and view a "metric set" of RMSE, MSE, and $R^2$ as shown:

```{r}
diamond_metrics <- metric_set(rmse, rsq, mae)
diamond_metrics(diamond_train_res, truth = price, 
                estimate = .pred)
diamond_metrics(diamond_test_res, truth = price,
                estimate = .pred)
```

#### Activities:

-   Is there a difference between the three metrics for the training data and the testing data?

-   Do your best to explain why this difference does (or does not) exist.

## *k*-Nearest Neighbors

Now we'll take the recipe we've already created and try fitting a KNN [(*k*-nearest neighbors)](https://parsnip.tidymodels.org/reference/nearest_neighbor.html) model with it! To do this, we'll use the `nearest_neighbor()` function, rather than the `linear_reg()` function, but we'll still need to select an engine. There is only one *R* package, or engine, that works with `nearest_neighbor()`, and that is the `kknn` package.

![](images/parsnip.png){width="363"}

To use an engine, we must make sure that the related package is installed and loaded on our machine. The code to do that is below -- however, remember that you must keep the `install_packages()` line commented out to successfully knit this file.

```{r}
# install.packages("kknn")
library(kknn)

knn_model <- nearest_neighbor() %>% 
  set_engine("kknn") %>% 
  set_mode("regression")
```

Unlike a linear regression, however, there is a parameter -- more specifically, a *hyperparameter* -- that we have to set to fit a *k*-nearest neighbors model. That hyperparameter is `k` -- the number of neighbors for the model to consider.

How do we know the "right" or the optimal value of `k` to use? Well, we don't! Eventually we'll discuss the concepts of resampling, cross-validation, and tuning, which will allow us to determine the optimal value of a hyperparameter with relative ease. For now, we'll go with the default value of `k`.

Also unlike a linear regression, we need to specify whether our model is for regression or classification using `set_mode()`. Simply by fitting a linear regression model, it's implied that the problem must be for regression, but KNN models can be used for either regression *or* classification.

#### Activities:

-   What IS the default value of `k` here, if we do not specify a value? How did you find out?

Now we add the model and recipe to the workflow and fit the model to the training data set:

```{r}
knn_wflow <- workflow() %>% 
  add_model(knn_model) %>% 
  add_recipe(simple_diamonds_recipe)

knn_fit <- fit(knn_wflow, diamonds_train)
```

Trying to view the results, as we did with the linear regression, is not very informative:

```{r}
knn_fit %>% 
  extract_fit_parsnip()
```

It tells us that the "best" value of `k` is 5, but that is also because it's the only value we tried, so it doesn't mean much.

We'll generate the predictions from this model for the training set and testing set, and then compare the metrics for each, as before:

```{r}
diamond_train_knn <- predict(knn_fit, new_data = diamonds_train %>% select(-price))
diamond_train_knn <- bind_cols(diamond_train_knn, diamonds_train %>% select(price))

diamond_test_knn <- predict(knn_fit, new_data = diamonds_test %>% select(-price))
diamond_test_knn <- bind_cols(diamond_test_knn, diamonds_test %>% select(price))

diamond_metrics(diamond_train_knn, truth = price, 
                estimate = .pred)
diamond_metrics(diamond_test_knn, truth = price,
                estimate = .pred)
```

Let's try a different value for *k*. We can manually specify a value by adding it inside the `nearest_neighbor()` function. Later, when we introduce cross-validation, that's also where we'll specify the parameter to be tuned. The value we try is arbitrary; we can try basically any value of *k* from 1 to *n*. Then we have to fit the new model, etc.:

```{r}
knn_model2 <- nearest_neighbor(neighbors = 2) %>% 
  set_engine("kknn") %>% 
  set_mode("regression")

knn_wflow2 <- workflow() %>% 
  add_model(knn_model2) %>% 
  add_recipe(diamonds_recipe)

knn_fit2 <- fit(knn_wflow2, diamonds_train)

diamond_train_knn2 <- predict(knn_fit2, new_data = diamonds_train %>% select(-price))
diamond_train_knn2 <- bind_cols(diamond_train_knn2, diamonds_train %>% select(price))

diamond_test_knn2 <- predict(knn_fit2, new_data = diamonds_test %>% select(-price))
diamond_test_knn2 <- bind_cols(diamond_test_knn2, diamonds_test %>% select(price))

diamond_metrics(diamond_train_knn2, truth = price, 
                estimate = .pred)
diamond_metrics(diamond_test_knn2, truth = price,
                estimate = .pred)
```

#### Activities:

-   Which of the three models -- linear regression, *k*-nearest neighbors with $k = 5$, or *k*-nearest neighbors with $k = 2$ -- performed better on the testing data? Why do you think this is so?

-   What do you think explains the difference between the training and testing metrics for the KNN models?

-   How many predictors are included in our KNN model(s)?

-   Do you think changing the number of predictors might improve the performance of the KNN model(s)?

## Missing Data

We'll use the `airquality` data set, which comes installed with base R, to illustrate some ways of handling missing data. Let's start by loading [the `naniar` package](https://cran.r-project.org/web/packages/naniar/vignettes/getting-started-w-naniar.html). Usually it's considered good practice to load all packages in the same place -- at the beginning of the Markdown file -- but we'll make an exception. If this is your first time using the package, you'll need to uncomment and run the `install.packages()` line.

Try running `?airquality` to learn about the variables in the data set. There are 153 observations on six variables.

It often makes sense to look at missingness in the data prior to splitting it. `vis_miss()` is a good function for this:

```{r}
# install.packages("naniar")
library(naniar)
vis_miss(airquality)
```

This plot shows us at a glance that 4.8% of the entire dataset is missing; that missingness is on two of six variables, `Ozone` and `Solar.R`. 24.16% of `Ozone` observations are missing and 4.56% of `Solar.R` observations are. We can see that these observations are missing also by directly looking at the data:

```{r}
airquality %>% 
  kable() %>% 
  kable_styling(full_width = F) %>% 
  scroll_box(width = "100%", height = "200px")
```

Since the data set is meant to measure air quality, it makes sense that we might want to predict the ozone levels. We can look at a histogram of the variable:

```{r}
ggplot(aes(x = Ozone), data = airquality) +
  geom_histogram()
```

If we tried to run this code in the R console -- outside of the .Rmd environment -- we would receive a warning message that `37 rows containing non-finite values` were removed. That is R's way of informing us that it handled the missing values, or `NA`s, by dropping those observations entirely. That message doesn't appear in the knitted .html file for this lab because in the global options chunk, at the very top of this document, we set `message=FALSE` and `warning=FALSE` for the sake of neatness.

Some R functions, like `geom_histogram()`, will handle missing data in one way or another by default and will give a reasonable response or value. Others will not:

```{r}
airquality %>% 
  summarise(mean(Ozone))
```

If there is even one `NA` value, for instance, `mean()` will report `NA`. We can tell it to drop any rows with `NA` and get a value instead:

```{r}
airquality %>% 
  summarise(mean(Ozone, na.rm = T))
```

One way we could choose to handle the missingness is to remove the variables with any missing data by simply not including them. However, here one of those variables is our outcome, and it would make very little sense to remove it.

Another way is to remove all observations with missing data from the dataset, which is what `geom_histogram()` and `mean()` do by default. We could do that before we split the data like so:

```{r}
airquality_droprow <- airquality %>% drop_na()
```

However, dropping those observations reduced the overall number of observations from 135 to 111, a reduction of approximately 17.78%, which is a fairly large reduction for a dataset that was already fairly small.

The other option is to use some form of imputation and generate values for the missing observations. There are several functions we can use for imputation; [you can find more about them here](https://recipes.tidymodels.org/reference/step_impute_linear.html). The primary downside to mean, median, and mode imputation are that they result in a reduction in variance. Imputation using KNN, bagging, or a linear model tends to perform better.

Since only two variables have missingness, we'll take a relatively easy route and use linear imputation to handle them. This can be incorporated in the recipe like so:

```{r}
airquality_split <- initial_split(airquality, prop = 0.80,
                                strata = Ozone)
airquality_train <- training(airquality_split)
airquality_test <- testing(airquality_split)

air_recipe <- recipe(Ozone ~ ., data = airquality_train) %>% 
  step_impute_linear(Ozone, impute_with = 
                       imp_vars(Wind, Temp, Month, Day)) %>% 
  step_impute_linear(Solar.R, impute_with = 
                       imp_vars(Wind, Temp, Month, Day))
```

And we can `prep()` and `bake()` the recipe to verify that it worked -- these missing values were imputed, or predicted, using the complete variables!

```{r}
prep(air_recipe) %>% 
  bake(new_data = airquality_train) %>% 
  kable() %>% 
  kable_styling(full_width = F) %>% 
  scroll_box(width = "100%", height = "200px")
```

`vis_miss()` on the prepped data would tell us no observations are missing now:

```{r}
prep(air_recipe) %>% 
  bake(new_data = airquality_train) %>% 
  vis_miss()
```

Note that you don't need to handle missingness on all variables in the same way. You could choose to use mean imputation for one, KNN imputation for another, drop a third completely, etc. The most important aspects here are that you should (a) handle **all** missingness in some way, (b) if possible consider **why** observations are missing, and (c) **report** what you did for the sake of transparency.

#### Activities

- Fit a linear regression model using the imputed data.

- Compare the results of that model to a linear regression fit using mean imputation for both variables. Discuss the similarities and differences.

## Resources

The free book [Tidy Modeling with R](https://www.tmwr.org/) is strongly recommended.

You can view all the ISLR textbook code written with `tidymodels` [here](https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/index.html).
