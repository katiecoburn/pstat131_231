---
title: "Lab 2"
author: "PSTAT 131/231"
output:
    html_document:
      toc: true
      toc_float: true
      code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

## Introduction: Model Fitting

```{r}
library(tidyverse)
library(tidymodels)
library(ggplot2)
library(corrplot)
library(ggthemes)
library(kableExtra)
tidymodels_prefer()

set.seed(3435)

diamonds_split <- initial_split(diamonds, prop = 0.80,
                                strata = price)
diamonds_train <- training(diamonds_split)
diamonds_test <- testing(diamonds_split)
```

### Creating a Recipe

You'll notice that the textbooks use the `lm()` function to fit a linear regression. The `tidymodels` framework, however, has its own structure and flow, which is designed to work with multiple different machine learning models and packages seamlessly.

To fit any model with `tidymodels`, the first step is to create a recipe. The structure of this recipe is similar to that of `lm()`; the outcome is listed first, then the features are added:

```{r}
simple_diamonds_recipe <-
  recipe(price ~ ., data = diamonds_train)
```

Note that `.` is a placeholder for "all other variables." If we call the recipe object now, we can see some information:

```{r}
simple_diamonds_recipe
```

More specifically, we see that there are 9 predictors. 

We should dummy-code all categorical predictors. We can do that easily with `step` functions:

```{r}
diamonds_recipe <- recipe(price ~ ., data = diamonds_train) %>% 
  step_dummy(all_nominal_predictors())
```

Note that we haven't specified what type of model we'll be fitting yet. The other beauty of the recipe is that it can then be directly given to one of many machine learning model "engines."

Running the above code is essentially like writing down the instructions for a recipe on a sheet of paper. We've prepared the recipe to give to the workflow, but we are probably interested in seeing what the results of the recipe itself actually look like. Did the dummy coding work, for example? To apply the recipe to a data set and view the results, we can use `prep()`, which is akin to setting out a *mise en place* of ingredients, and `bake()`.

We'll also use `kbl()` and `kable_styling()` from the [`kableExtra` package](https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html), which we installed above. It's not necessary to use these functions, but doing so allows the table of data to display more neatly, so that all the columns and rows are actually legible. We also use `head()` to select only the first few rows; otherwise the entire data frame would print, which would be time-consuming. Also note the use of `scroll_box()`, allowing us to scroll through the entire data set.

```{r}
prep(diamonds_recipe) %>% 
  bake(new_data = diamonds_train) %>% 
  head() %>% 
  kable() %>% 
  kable_styling(full_width = F) %>% 
  scroll_box(width = "100%", height = "200px")
```


#### Activities:

- Use the Internet to find documentation about the possible `step` functions. Name three `step` functions that weren't used here and describe what they do.

## Linear Regression

Next, we can specify the model engine that we want to fit:

```{r}
lm_model <- linear_reg() %>% 
  set_engine("lm")
```

We set up a workflow. This step might seem unnecessary now, with only one model and one recipe, but it can make life easier when you are trying out a series of models or several different recipes later on.

```{r}
lm_wflow <- workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(diamonds_recipe)
```

Finally, we can fit the linear model to the training set:

```{r}
lm_fit <- fit(lm_wflow, diamonds_train)
```

We can view the model results:

```{r}
lm_fit %>% 
  # This returns the parsnip object:
  extract_fit_parsnip() %>% 
  # Now tidy the linear model object:
  tidy()
```

#### Activities:

- Explain what the intercept represents. 

- Describe the effect of `carat`. Is it a significant predictor of `price`? Holding everything else constant, what is the effect on `price` of a one-unit increase in `carat`?

The following code generates predicted values for `price` for each observation in the training set:

```{r}
diamond_train_res <- predict(lm_fit, new_data = diamonds_train %>% select(-price))
diamond_train_res %>% 
  head()
```

Now we attach a column with the actual observed `price` observations:

```{r}
diamond_train_res <- bind_cols(diamond_train_res, diamonds_train %>% select(price))
diamond_train_res %>% 
  head()
```

We might be interested in a plot of predicted values vs. actual values, here for the training data:

```{r}
diamond_train_res %>% 
  ggplot(aes(x = .pred, y = price)) +
  geom_point(alpha = 0.2) +
  geom_abline(lty = 2) + 
  theme_bw() +
  coord_obs_pred()
```

It's fairly clear that the model didn't do very well. If it predicted every observation accurately, the dots would form a straight line. We also have predicted some negative values for price, and once the actual price is approximately over $\$5,000$, the model does a pretty poor job.

The odds are that a linear model is simply not the best tool for this machine learning task. It is likely not an accurate representation of `f()`; remember that by using a linear regression, we are imposing a specific form on the function, rather than learning the function from the data.

In future labs, we'll try out different models and compare them. Finally, we can calculate the **training** root mean squared error (RMSE) and the **testing** RMSE.

```{r}
diamond_test_res <- predict(lm_fit, new_data = diamonds_test %>% select(-price))
diamond_test_res <- bind_cols(diamond_test_res, diamonds_test %>% select(price))
  
rmse(diamond_train_res, truth = price, estimate = .pred)
rmse(diamond_test_res, truth = price, estimate = .pred)
```

We can create and view a "metric set" of RMSE, MSE, and $R^2$ as shown:

```{r}
diamond_metrics <- metric_set(rmse, rsq, mae)
diamond_metrics(diamond_train_res, truth = price, 
                estimate = .pred)
diamond_metrics(diamond_test_res, truth = price,
                estimate = .pred)
```

#### Activities:

- Is there a difference between the three metrics for the training data and the testing data?

- Do your best to explain why this difference does (or does not) exist.

## *k*-Nearest Neighbors

Now we'll take the recipe we've already created and try fitting a KNN [(*k*-nearest neighbors)](https://parsnip.tidymodels.org/reference/nearest_neighbor.html) model with it! To do this, we'll use the `nearest_neighbor()` function, rather than the `linear_reg()` function, but we'll still need to select an engine. There is only one *R* package, or engine, that works with `nearest_neighbor()`, and that is the `kknn` package.

![](images/parsnip.png){width="363"}

To use an engine, we must make sure that the related package is installed and loaded on our machine. The code to do that is below -- however, remember that you must keep the `install_packages()` line commented out to successfully knit this file.

```{r}
# install.packages("kknn")
library(kknn)

knn_model <- nearest_neighbor() %>% 
  set_engine("kknn")
```

Unlike a linear regression, however, there is a parameter -- more specifically, a *hyperparameter* -- that we have to set to fit a *k*-nearest neighbors model. That hyperparameter is `k` -- the number of neighbors for the model to consider.

How do we know the "right" or the optimal value of `k` to use? Well, we don't! Eventually we'll discuss the concepts of resampling, cross-validation, and tuning, which will allow us to determine the optimal value of a hyperparameter with relative ease. For now, we'll go with the default value of `k`.

#### Activities:

- What IS the default value of `k` here, if we do not specify a value? How did you find out?

Now we add the model and recipe to the workflow and fit the model to the training data set:

```{r}
knn_wflow <- workflow() %>% 
  add_model(knn_model) %>% 
  add_recipe(diamonds_recipe)

knn_fit <- fit(knn_wflow, diamonds_train)
```

Trying to view the results, as we did with the linear regression, is not very informative:

```{r}
knn_fit %>% 
  extract_fit_parsnip()
```

It tells us that the "best" value of `k` is 5, but that is also because it's the only value we tried, so it doesn't mean much.

We'll generate the predictions from this model for the training set and testing set, and then compare the metrics for each, as before:

```{r}
diamond_train_knn <- predict(knn_fit, new_data = diamonds_train %>% select(-price))
diamond_train_knn <- bind_cols(diamond_train_knn, diamonds_train %>% select(price))

diamond_test_knn <- predict(knn_fit, new_data = diamonds_test %>% select(-price))
diamond_test_knn <- bind_cols(diamond_test_knn, diamonds_test %>% select(price))

diamond_metrics(diamond_train_knn, truth = price, 
                estimate = .pred)
diamond_metrics(diamond_test_knn, truth = price,
                estimate = .pred)
```

#### Activities:

- Which of the two models -- linear regression or *k*-nearest neighbors -- performed better on the testing data? Why do you think this is so?

- What do you think explains the difference between the training and testing metrics for the KNN model?

## Resources

The free book [Tidy Modeling with R](https://www.tmwr.org/) is strongly recommended.

You can view all the ISLR textbook code written with `tidymodels` [here](https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/index.html).